import inference

import os
import sys
import json
import torch
import spacy
import pathlib
import inflect 
import argparse
import sng_parser 
import subprocess

from glob import glob
from BLIP.models.blip_vqa import blip_vqa
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from big_phoney.big_phoney import BigPhoney

print("\n-- START --")

parser = argparse.ArgumentParser(description='VIIDA')

parser.add_argument('--path_dataset', dest='path_dataset', type=str, default='./webinar_dataset/', help="Path to the image dataset.")

parser.add_argument('--img_format', dest='img_format', type=str, default='.jpg', help=" Image file format (e.g., .jpg, .png)") 

parser.add_argument('--dense_captioning', dest='dense_captioning', type=bool, default=False, help="Set this to True if you used a dense captioning model.")

parser.add_argument('--model_vqa_path', dest='model_vqa_path', type=str, default='./BLIP/model_base_vqa_capfilt_large.pth', help="Path to the BLIP-VQA pth file.")

parser.add_argument('--ttext_sim', dest='ttext_sim', type=float, default=0.55, help="Text similarity threshold to filter similar dense captions generated by the dense captioning model.")

parser.add_argument('--ttext_yes', dest='ttext_yes', type=float, default=0.6, help="Similarity threshold for validating the content described in captions using the VQA model.")

parser.add_argument('--dense_model', dest='dense_model', type=str, default='none', help="Name of the dense captioning model used (e.g., densecap, grit, or none if no model was used).")

parser.add_argument('--dense_file', dest='dense_file', type=str, default='./my_results.json', help="Path to the .json file containing extracted dense captions.") 

args = parser.parse_args()

path_dts       = args.path_dataset
images_format  = args.img_format
dense_used     = args.dense_captioning
model_path     = args.model_vqa_path
threshold_sim  = args.ttext_sim
threshold_yes  = args.ttext_yes
densecap_model = args.dense_model
data_file      = args.dense_file

list_ids = []

if dense_used:
    with open(data_file) as json_file:
        dense_captions_extracted = json.load(json_file)
        
    if densecap_model == 'densecap': 
        for j, i in enumerate(dense_captions_extracted['results']):
            id_out = i['img_name']
            list_ids.append(id_out)
            
    elif densecap_model == 'grit':
        for j, i in enumerate(dense_captions_extracted):
            id_out = i['image_id']
            list_ids.append(id_out)
            
    else: #'none' case
        ...
        
else: 
    dense_captions_extracted = ""
    
    images_list = glob(os.path.join(path_dts, "*" + images_format))
    
    for img in images_list:
      file = pathlib.Path(img)
      list_ids.append(file.name)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("\nDevice: ", device)

print("\nLoading models...")

model_name = "en_core_web_lg"

try:
    nlp = spacy.load(model_name)
except OSError:
    subprocess.run(["python", "-m", "spacy", "download", model_name], check=True)
    nlp = spacy.load(model_name)

nlp.add_pipe('universal_sentence_encoder')
parser = sng_parser.Parser('spacy', model=model_name)

phoney = BigPhoney()
inflect = inflect.engine()

MODEL_NAME = "textattack/roberta-base-CoLA"
tokenizer_roberta = AutoTokenizer.from_pretrained(MODEL_NAME)
model_roberta = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

model_vqa = blip_vqa(pretrained=model_path, image_size=480, vit='base')
model_vqa.eval()
model_vqa = model_vqa.to(device)

annotations_list = []

questions = ["Is the person a man or woman?",
            "How old is that person?",
            "What is the person's skin color?",
            "What color is the eyes?",
            "Is the person bald?",
            "the person's hair is short or long?",
            "What color is the person's hair?",
            "Have a mustache?",
            "What color is the mustache?",
            "Have a beard?",
            "What color is the beard?",
            "How is the beard?",
            "Does the person wear makeup?",
            "Is the person wearing any accessories?",
            "What accessory is the person wearing?",
            "Accessory colors?", #this question is changed later
            "What is the person's facial expression?",
            "What top clothes is the person wearing?",
            "Does the top clothes have more than one color?",
            "Clothes colors?", #this question is changed later
            "What environment is the person in?",
            "What objects are behind the person?",
            "Are the person's eyes open?"]

print("\nRunning VIIDA...\n")
for num, img in enumerate(list_ids[:2]):

    print("Image %d: %s" % (num+1, img))

    inference.main([img], questions, threshold_sim, threshold_yes, path_dts, 
         device, nlp, parser, phoney, inflect, 
         tokenizer_roberta, model_roberta, model_vqa, 
         annotations_list, densecap_model, dense_captions_extracted, dense_used)
    
print("-- FINISHED --")   
